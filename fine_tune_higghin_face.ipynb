{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: evaluate in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: filelock in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages/huggingface_hub-0.12.0-py3.8.egg (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m118.9/118.9 kB\u001B[0m \u001B[31m663.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: lxml in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from sacrebleu) (4.9.1)\n",
      "Requirement already satisfied: regex in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from sacrebleu) (2022.7.9)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tomer/anaconda3/envs/nlp_ex4/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\n",
      "Installing collected packages: tabulate, portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39d8d5e8fed4d6baac8438c08ac3feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d4f904d5ba4c6bb117b4f976a07896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/161k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac72ebd1a604a8ab17297151f34f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/20.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset opus_books/en-fr to /home/tomer/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d81ed61449498680ac4862307486eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset opus_books downloaded and prepared to /home/tomer/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830f9dc379d342acba00528d153a76a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '121613',\n",
       " 'translation': {'en': \"The wind blew between the planks of chestnut and oak, and they rolled themselves up as in some wood-cutter's abandoned hut.\",\n",
       "  'fr': 'Le vent sifflait entre les perches de châtaignier et de chene, ils se pelotonnaient, comme dans une hutte de bucheron abandonnée.'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d56d989b7cb4bd3b2345a6379723849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/tomer/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf/cache-191295a3d1332415.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033be300730b41228fe0813fbb2615d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b4dc14db04425b88c8ef0640c06216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: translation, id. If translation, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 101668\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25418\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11077' max='25418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11077/25418 20:33 < 26:36, 8.98 it/s, Epoch 0.87/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-1000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-1000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-1000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-1500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-1500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-1500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-2000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-2000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-2000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-2500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-2500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-2500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-3000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-3000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-3000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-3500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-3500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-3500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-4000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-4000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-4000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-4500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-4500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-4500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-5000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-5000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-5000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-5500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-5500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-5500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-6000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-6000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-6000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-6500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-6500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-6500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-7000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-7000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-7000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-7000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-7500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-7500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-7500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-8000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-8000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-8000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-8500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-8500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-8500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-9000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-9000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-9000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-9500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-9500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-9500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-10000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-10000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-10000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-10500\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-10500/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-10500/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to my_awesome_opus_books_model/checkpoint-11000\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-11000/config.json\n",
      "Configuration saved in my_awesome_opus_books_model/checkpoint-11000/generation_config.json\n",
      "Model weights saved in my_awesome_opus_books_model/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in my_awesome_opus_books_model/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_opus_books_model/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [my_awesome_opus_books_model/checkpoint-9500] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
